{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "OnEqCphqxYY3",
   "metadata": {
    "id": "OnEqCphqxYY3"
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> Predicting Hospital Readmission Rates for Diabetes\n",
    "## CS109a: Introduction to Data Science\n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2023**<br/>\n",
    "**Team members**: Karim Gowani, Ryan McGillicuddy, Yaseen Mohmand, Steven Worthington\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MSayYpjlHKPg",
   "metadata": {
    "id": "MSayYpjlHKPg",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ByqZTSs3GN1A",
   "metadata": {
    "id": "ByqZTSs3GN1A"
   },
   "source": [
    "## i. About This Notebook\n",
    "\n",
    "This notebook represents a summary of a series of exploratory analysis, wherein we attempt various approaches to classify X-ray images into 14 disease classes and a 'no finding' class. For an overview of the analytical sections of this notebook, please see the [Notebook Contents](#Notebook-Contents) index listed below this Introduction.\n",
    "\n",
    "### i.i. A note about supporting notebooks\n",
    "\n",
    "The code and results reported in this notebook are only a summary of the work completed for this project - they represent the final form of our analyses. Supplemental notebooks containing auxillary EDA, data cleaning, and model exploration illustrated in this report can be found in the **`notebooks/`** directory of the **[GitHub project repository](https://github.com/liujinjie111/chestXray)**. The notebooks in this repository are not designed to be run in any particular order to reproduce the results shown in the final report. They just show the extra work and experiments we have tried that are not shown in the final report.\n",
    "\n",
    "## ii. Research Question\n",
    "\n",
    "After initial exploration and cleaning of the data, we have focused our efforts on the following research question:\n",
    "\n",
    "Which model architecture performs best for out-of-sample classification of the X-ray images into the 14 disease classes and 'no finding' class?\n",
    "\n",
    "## iii. Summary of Findings\n",
    "\n",
    "We found this project to be both interesting and quite challenging. We created an analysis pipeline using TF datasets that was efficient enough for us to experiment with many different modeling architectures in a short period of time. This encorporated downsampling of the majority image label classes to make modeling more tractable. We incorporated data augmentation steps into the model itself to use GPU, rather than CPU, cycles, thus reducing the computational burden and time cost of preprocessing, allowing us to devote more time to exploring different modelling approaches. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54_zEcgbHupp",
   "metadata": {
    "id": "54_zEcgbHupp"
   },
   "source": [
    "<a name=\"Notebook-Contents\"></a>\n",
    "# Notebook Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oRrQSWoyHjbX",
   "metadata": {
    "id": "oRrQSWoyHjbX"
   },
   "source": [
    "[Introduction](#Introduction)\n",
    "\n",
    "[Setup](#Setup)\n",
    "\n",
    "**[1. Data Source and Composition](#1.-Data-Source-and-Composition)**\n",
    "\n",
    "- [1.1. Data Source and Substantive Context](#1.1.-Data-Source-and-Substantive-Context)\n",
    "\n",
    "- [1.2. Data Granularity](#1.2.-Data-Granularity)\n",
    "\n",
    "- [1.3. Class Imbalance](#1.3.-Class-Imbalance)\n",
    " \n",
    "- [1.4. Missing Observations](#1.4.-Missing-Observations)\n",
    "  \n",
    "**[2. Exploratory Data Analysis and Preprocessing](#2.-Exploratory-Data-Analysis-and-Preprocessing)**\n",
    "\n",
    "- [2.1. Exploratory Data Analysis of Raw Data](#2.1.-Exploratory-Data-Analysis-of-Raw-Data)\n",
    "\n",
    "- [2.2. Data Preprocessing](#2.2.-Data-Preprocessing)\n",
    "\n",
    "- [2.3. Data Partitioning](#2.3.-Data-Partitioning)\n",
    " \n",
    "- [2.4. Exploratory Data Analysis of Cleaned Data](#2.4.-Exploratory-Data-Analysis-of-Cleaned-Data)\n",
    "\n",
    "- [2.5. Summary of EDA Key Findings](#2.5.-Summary-of-EDA-Key-Findings)\n",
    "\n",
    "**[3. Research Questions](#3.-Research-Questions)**\n",
    "\n",
    "**[4. Modeling Pipeline and Training](#4.-Modeling-Pipeline-and-Training)**\n",
    "\n",
    "- [4.1. Candidate Models](#4.1.-Candidate-Models)\n",
    "\n",
    "- [4.2. Hyperparameter Tuning Settings](#4.2.-Hyperparameter-Tuning-Settings)\n",
    "\n",
    "- [4.3. Performance Metrics](#4.3.-Performance-Metrics)\n",
    "\n",
    "- [4.4. Resampling Scheme](#4.4.-Resampling-Scheme)\n",
    "\n",
    "- [4.5. Model Training](#4.5.-Model-Training)\n",
    "\n",
    "**[5. Model Selection and Evaluation](#5.-Model-Selection-and-Evaluation)**\n",
    "\n",
    "- [5.1. Model Selection](#5.1.-Model-Selection)\n",
    "\n",
    "- [5.2. Best Model Performance](#5.2.-Best-Model-Performance)\n",
    "\n",
    "- [5.3. Variable Importance](#5.3.-Variable-Importance)\n",
    "\n",
    "**[6. Conclusions](#6.-Conclusions)**\n",
    "\n",
    "- [6.1. Patient Early Readmittance Rate](#6.1.-Patient-Early-Readmittance-Rate)\n",
    "\n",
    "- [6.2. Patient Risk Profiles](#6.2.-Patient-Risk-Profiles)\n",
    "\n",
    "**[7. Future Work](#7.-Future-Work)**\n",
    "\n",
    "**[8. References](#8.-References)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hHP1gK8HFsIi",
   "metadata": {
    "id": "hHP1gK8HFsIi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jkSKoN5ycDBU",
   "metadata": {
    "id": "jkSKoN5ycDBU"
   },
   "source": [
    "The following sections include general setup code for:\n",
    "1. Installing the necessary packages needed for data preparation, modeling, and visualization\n",
    "2. Setting pseudo-random number seeds for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28arK-Bz3Y0X",
   "metadata": {
    "id": "28arK-Bz3Y0X"
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5ebd5-787f-4930-9e3e-97f620170a85",
   "metadata": {
    "id": "15c5ebd5-787f-4930-9e3e-97f620170a85"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import r2_score, confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-notebook')\n",
    "sns.set_style('darkgrid')\n",
    "# pandas tricks for better display\n",
    "pd.options.display.max_columns = 50  \n",
    "pd.options.display.max_rows = 500     \n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.precision = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ApQw4ga3kJ_",
   "metadata": {
    "id": "8ApQw4ga3kJ_"
   },
   "source": [
    "### Set RNG seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NBxFxxPbgaSE",
   "metadata": {
    "id": "NBxFxxPbgaSE"
   },
   "outputs": [],
   "source": [
    "# Ensure replicable results\n",
    "import os\n",
    "import random as rn\n",
    "SEED = 109\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ij7PyqYbw7-E",
   "metadata": {
    "id": "ij7PyqYbw7-E",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"1.-Data-Source-and-Composition\"></a>\n",
    "# 1. Data Source and Composition\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G2yE0i7VZ6gA",
   "metadata": {
    "id": "G2yE0i7VZ6gA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"1.1.-Data-Source-and-Substantive-Context\"></a>\n",
    "## 1.1 Data Source and Substantive Context\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdec518-019b-4e69-960f-749f0370a3a1",
   "metadata": {},
   "source": [
    "Our dataset is from the UC Irvine ML Repository and involves patient records of those diagnosed with diabetes from 1999 through 2008 at 130 US hospitals. We have downloaded this dataset and examined it. It has ~102K records, a binary target variable, and 47 features, the majority of which are categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98aee9-3159-4f32-aab7-7d7e68b61c4f",
   "metadata": {},
   "source": [
    "<a name=\"1.2.-Data-Granularity\"></a>\n",
    "## 1.2 Data Granularity\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82336ab6-210e-4811-b5d6-bfe56ddd6c2c",
   "metadata": {},
   "source": [
    "In a clinical setting, doctors and medical staff would like to answer the question, “given information from the current and previous hospitalizations, how likely is it for this patient to be readmitted to hospital early (within 30 days)?”. This question is inherently at the patient-level, but each record in the dataset is at the level of an ‘encounter’, which represents a patient hospitalization event (rather than an outpatient visit). A subset of 16.5% of patients have multiple encounters.\n",
    "\n",
    "A patient-level perspective is more likely to be of benefit to clinicians, since answering the above question will help medical personnel prioritize follow-ups and interventions through the creation of patient risk profiles, which can identify patients at the highest risk level for early hospital readmittance. This information is actionable and can be used to mitigate negative health outcomes for these patients as well as increased costs for the hospital and insurance carrier. We will therefore aggregate data from the encounter-level to the patient-level.\n",
    "\n",
    "For those patients with multiple encounters, however, features that vary at the encounter-level contain important information that we do not wish to discard. For example, if a patient was readmitted early relative to the immediately preceding encounter, it is perhaps more likely that the patient will be readmitted to hospital early again after the current encounter. Therefore, our strategy will be to select only the final encounter for these patients and create several new derived features that encapsulate the history of their previous encounters. Such features will include, but are not limited to, the number of previous inpatient encounters, whether the last encounter resulted in early readmission, and whether the patient ever had a high value of A1c.\n",
    "\n",
    "In following this approach, we will have to make a (reasonable) assumption that encounters for each patient are in temporal order in the dataset because no explicit date information is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c379f56b-763f-4e78-945a-20ee58ea4df1",
   "metadata": {},
   "source": [
    "<a name=\"1.3.-Class-Imbalance\"></a>\n",
    "## 1.3 Class Imbalance\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e118a-a2f8-4dbe-b940-dcb764baa4e4",
   "metadata": {},
   "source": [
    "About 11% of encounters belong to the positive class (readmitted within 30 days), so while there is imbalance, it is not severe. While our performance metric of interest - AUC - is robust to class imbalance, we will still try to address this issue in several ways. We will use stratified sampling in train/test splits, and we will attempt the standard techniques of undersampling and oversampling, as well as use of class weights built into the different ML models of interest, including Logistic Regression, CART, Random Forest, and XG Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892a96b-6e50-4d07-bcfe-efcf0f5ea129",
   "metadata": {},
   "source": [
    "<a name=\"1.4.-Missing-Observations\"></a>\n",
    "## 1.4 Missing Observations\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3e9b7-e37e-4723-88e5-301a51f86225",
   "metadata": {},
   "source": [
    "There are only 7 (out of 47) relevant columns that contain missing values:\n",
    "- weight is missing ~97% of its values, so this column can be safely dropped; no other numerical column has missing values.\n",
    "- medical specialty is missing ~49% of its values, but may be relevant to the classification task, so we keep it and fill the missing values with ‘unknown’..\n",
    "- payer code (insurance carrier) is missing ~40% of the values, but because it does not seem to be relevant to the target - it is also a candidate for being eliminated altogether.\n",
    "\n",
    "The remaining columns have less than 3% values missing so they can be managed. They are all categorical, including race and diagnosis codes. As is common for categorical variables, we will fill the missing values with 'unknown'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72309a08-c74e-402a-8ee4-b9d2b57451ca",
   "metadata": {
    "id": "ij7PyqYbw7-E",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"2.-Exploratory-Data-Analysis-and-Preprocessing\"></a>\n",
    "# 2. Exploratory Data Analysis and Preprocessing\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pz8rNYJtQfR4",
   "metadata": {
    "id": "pz8rNYJtQfR4",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"2.1.-Exploratory-Data-Analysis-of-Raw-Data\"></a>\n",
    "## 2.1 Exploratory Data Analysis of Raw Data\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722b44b-a09f-4fb8-9db1-cc4e978dc69a",
   "metadata": {
    "id": "GXAwzxT_zzgu",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Raw Diabetes Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea023f8-41de-43f8-8da3-885554233b01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"2.2.-Data-Preprocessing\"></a>\n",
    "## 2.2 Data Preprocessing\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aGi0EGNLXvHY",
   "metadata": {
    "id": "aGi0EGNLXvHY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Overall Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71a9c9-03c3-4846-ad5f-aa17b3d18a95",
   "metadata": {},
   "source": [
    "Several of the categorical variables have many categories each that should be easily collapsed to reduce dimensionality:\n",
    "- At the most extreme, the first diagnosis column has 716 codes, only 23 of which represent more than 1% of the observations; similarly for the second and third diagnosis variables. In fact, these diagnosis codes should be grouped into types such as Circulatory (codes 390-459), respiratory (codes 460-519), digestive (520-579), etc.\n",
    "- Admission type code has 8 categories, 3 of which make up less than 1% of observations and can be safely collapsed.\n",
    "- Medical specialty has 72 categories, but only 9 represent more than 1% of the observations.\n",
    "- Age buckets can be consolidated: Currently, each bucket includes only 10 years. Less than 1% of\n",
    "the observations fall into age < 20 and age > 90, for instance.\n",
    "\n",
    "Furthermore, patients who were discharged with codes such as expired, hospice, transferred to another institution as inpatient, etc. should be filtered out as these types of discharge codes are of no practical relevance for predicting the target of early readmission. Trivially, encounter ID and patient ID are mere identifiers and should not be fed into any modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HxeAw_3CfgxD",
   "metadata": {
    "id": "HxeAw_3CfgxD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### One-hot encode labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mk3_GxQnZimp",
   "metadata": {
    "id": "mk3_GxQnZimp"
   },
   "source": [
    "**Findings**\n",
    "\n",
    "There are a total of 112,120 unique image filenames in the meta-data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K9SvwtD1Z0c-",
   "metadata": {
    "id": "K9SvwtD1Z0c-"
   },
   "source": [
    "**Findings**\n",
    "\n",
    "The majority of patients (\\~54%) have no evidence of disease, while \\~28% have been diagnosed with a single disease, and \\~19% have been diagnosed with multiple diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wbHx0rpaZ6II",
   "metadata": {
    "id": "wbHx0rpaZ6II"
   },
   "source": [
    "**Findings**\n",
    "\n",
    "While the majority of X-ray images show either no disease finding (\\~60K) or a single disease (\\~30K), there is a long right tail to the distribution. Some images have as many as 9 out of the possible 14 disease labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3q0gv1__Z8U4",
   "metadata": {
    "id": "3q0gv1__Z8U4"
   },
   "source": [
    "**Findings**\n",
    "\n",
    "Occurence differs dramatically among different diseases. Several diseases (e.g., hernia, pneumonia, fibrosis) have only a few hundred occurences in the X-ray images, while others (e.g., infiltration, effusion, atelectasis) have over 10,000 occurences. This means that the data exhibit extreme class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b392e-58b0-453e-ae67-53ff3706f867",
   "metadata": {},
   "source": [
    "<a name=\"2.3.-Data-Partitioning\"></a>\n",
    "## 2.3 Data Partitioning\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a59eb-94e2-49ce-8831-d3706a7bd608",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"2.4.-Exploratory-Data-Analysis-of-Cleaned-Data\"></a>\n",
    "## 2.4 Exploratory Data Analysis of Cleaned Data\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a4b5ab-d233-4491-85ef-cf42fe916ee1",
   "metadata": {
    "id": "id82KxlHfmai",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Disease occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c0524-0c32-4fe5-b14a-81e513df4df7",
   "metadata": {
    "id": "7YbF0RI5fvzS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Disease distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea24343-9fea-4564-9d6c-a92183e16b21",
   "metadata": {
    "id": "FOoRrPJJf0Kd",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Commonly occurring diseases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ffdd0-db31-4fb6-a7c0-676940339f1b",
   "metadata": {
    "id": "OK9Di01Yfqqd"
   },
   "source": [
    "### Disease correlations (co-morbidity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771yzHV0Z9L4",
   "metadata": {
    "id": "771yzHV0Z9L4",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"2.5.-Summary-of-EDA-Key-Findings\"></a>\n",
    "## 2.5 Summary of EDA Key Findings\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8jhMS7PaD4j",
   "metadata": {
    "id": "a8jhMS7PaD4j"
   },
   "source": [
    "After exploring the image data and the disease class labels, we have identified 5 major issues that will need to be addressed during data pre-processing and analysis.\n",
    "\n",
    "1. Placeholder1\n",
    "2. Placeholder2\n",
    "3. Placeholder2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7PvEAoyzKaTV",
   "metadata": {
    "id": "7PvEAoyzKaTV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"3.-Research-Questions\"></a>\n",
    "# 3. Research Questions\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LWHhIxQRbIEy",
   "metadata": {
    "id": "LWHhIxQRbIEy"
   },
   "source": [
    "After initial exploration and cleaning of the data, we have focused our efforts on answering the following research questions:\n",
    "\n",
    "1. **How likely are patients to be readmitted to hospital within 30 days of discharge?**\n",
    "\n",
    "2. **What risk factors drive early readmittance (within 30 days of discharge) to hospital?**\n",
    "\n",
    "Answering these questions will help medical personnel prioritize follow-ups and interventions through the creation of patient risk profiles, which can identify patients at the highest risk level for early hospital readmittance. This information is actionable and can be used to mitigate negative health outcomes for these patients as well as increased costs for the hospital and insurance carrier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ArpOmjDRZn",
   "metadata": {
    "id": "a3ArpOmjDRZn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"3.1.-Raw-Data\"></a>\n",
    "## 3.1 Raw Data\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ahls7odbRlDv",
   "metadata": {
    "id": "Ahls7odbRlDv"
   },
   "source": [
    "Here we load the images from the train, validation, and test sets into 3 separate TF datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gMx2r3vNlyIi",
   "metadata": {
    "id": "gMx2r3vNlyIi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"3.2.-Training-Data\"></a>\n",
    "## 3.2 Training Data\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WC1kWzdlwCo0",
   "metadata": {
    "id": "WC1kWzdlwCo0"
   },
   "source": [
    "### Select one-hot encoded labels for train, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U8tu5fl8Vinl",
   "metadata": {
    "id": "U8tu5fl8Vinl"
   },
   "source": [
    "Here we subset the one-hot encoded labels into the same train, validation, and test sets as the images, using the image filenames as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26Hm6oTJ1IB1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26Hm6oTJ1IB1",
    "outputId": "44eee496-cb8e-448f-bdd8-de3313b6b8f7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6Ab1GYCWlcE_",
   "metadata": {
    "id": "6Ab1GYCWlcE_"
   },
   "source": [
    "### Load one-hot encoded labels into TF datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jY1PCmFOV7M_",
   "metadata": {
    "id": "jY1PCmFOV7M_"
   },
   "source": [
    "Here we load the partitioned one-hot encoded label data into 3 TF datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94kqg4xT1kV2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94kqg4xT1kV2",
    "outputId": "72eca71e-bb8a-4703-af37-86113a39ceee"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "LGosJaq5l3PC",
   "metadata": {
    "id": "LGosJaq5l3PC",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"3.3.-Testing-Data\"></a>\n",
    "## 3.3 Testing Data\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jatOAfx11ax9",
   "metadata": {
    "id": "jatOAfx11ax9"
   },
   "source": [
    "### Combine images and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ggXvN9HIWFQp",
   "metadata": {
    "id": "ggXvN9HIWFQp"
   },
   "source": [
    "Here we combine the image and label TF datasets, by zipping them together to form 3 new TF datasets with both image and label information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XLp6ptSb1ZDv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLp6ptSb1ZDv",
    "outputId": "427de30c-9f8d-4cd0-fcf4-51fee8787d43"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ymeShx6UwUcr",
   "metadata": {
    "id": "ymeShx6UwUcr"
   },
   "source": [
    "### Batching & prefetching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0m_sNkHeWg90",
   "metadata": {
    "id": "0m_sNkHeWg90"
   },
   "source": [
    "Here we set up the batched TF datasets with prefetching that is autotuned. We used fairly small batch sizes to reduce memory demands and allow us to train more complex models with a larger sample of data. We shuffled the data for the training set to prevent the model from learning image order information, but not the validation or test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314f1ef-ac95-4271-8e5f-852ac51319cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4314f1ef-ac95-4271-8e5f-852ac51319cb",
    "outputId": "25b1217f-0fe7-43d5-be9d-0b703e9bb686"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "gqCxFvbTwZxy",
   "metadata": {
    "id": "gqCxFvbTwZxy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"4.-Modeling-Pipeline-and-Training\"></a>\n",
    "# 4. Modeling Pipeline and Training\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28175d-6c62-4026-a82e-a36567ffa6af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"4.1.-Candidate-Models\"></a>\n",
    "## 4.1 Candidate Models\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba371b9a-554e-4b1d-aa11-015543ba21a3",
   "metadata": {},
   "source": [
    "We have four candidate models:\n",
    "\n",
    "1. Logistic regression with L1 regularization\n",
    "2. Single Decision Tree\n",
    "3. Random Forest\n",
    "4. Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-mNiZYErwdXr",
   "metadata": {
    "id": "-mNiZYErwdXr"
   },
   "source": [
    "<a name=\"4.2.-Hyperparamter-Tuning-Settings\"></a>\n",
    "## 4.2 Hyperparameter Tuning Settings\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hPwMV4lFC3_Y",
   "metadata": {
    "id": "hPwMV4lFC3_Y"
   },
   "source": [
    "<a name=\"4.3.-Performance-Metrics\"></a>\n",
    "## 4.3 Performance Metrics\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de45f1e-eb46-498d-b45b-e9115d98e540",
   "metadata": {},
   "source": [
    "<a name=\"4.4.-Resampling-Scheme\"></a>\n",
    "## 4.4 Resampling Scheme\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f69457f-39b8-4b2a-bf95-3525672c7ec4",
   "metadata": {},
   "source": [
    "Repeated $k$-fold CV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HHEAeTmRFV0I",
   "metadata": {
    "id": "HHEAeTmRFV0I"
   },
   "source": [
    "<a name=\"4.5.-Model-Training\"></a>\n",
    "## 4.5 Model Training\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fpaswuaRzQr-",
   "metadata": {
    "id": "fpaswuaRzQr-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"5.-Model-Selection-and-Evaluation\"></a>\n",
    "# 5. Model Selection and Evaluation\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cH0u3yxItM",
   "metadata": {
    "id": "19cH0u3yxItM"
   },
   "source": [
    "<a name=\"5.1.-Model-Selection\"></a>\n",
    "## 5.1 Model Selection\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HCIrjzjtxM2I",
   "metadata": {
    "id": "HCIrjzjtxM2I",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"5.2.-Best-Model-Performance\"></a>\n",
    "## 5.2 Best Model Performance\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AnwZ2441uWPj",
   "metadata": {
    "id": "AnwZ2441uWPj"
   },
   "source": [
    "We generate predicted probabilities of class membership for all 14 diseases and the 'no finding' class on the test set. We then calculate the following metrics:\n",
    "1. Accuracy\n",
    "2. Prevalence  \n",
    "3. Sensitivity\n",
    "4. Specificity\n",
    "5. Positive Predictive Value      \n",
    "6. Negative Predictive Value  \n",
    "7. Area Under ROC \n",
    "8. AP Score  \n",
    "9. F1 Score\n",
    "\n",
    "Among these metrics, only Area Under ROC, AP Score, and F1 Score, do not depend on the choice of a threshold. Also, Accuracy, Prevalence, Positive Predictive Value, and Negative Predictive Value are not appropriate for evaluating class imbalance dataset, while Sensitivity, Specificity, Area Under ROC, AP Score, and F1 Score are less sensitive to class imbalance. Therefore, we have focused more on the metrics of Area Under ROC, AP Score, and F1 Score, since they are not subject to the arbitrary choice of a threshold value and are more informative performance metrics when significant class imbalance is present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BDFxlAHQ7h-Y",
   "metadata": {
    "id": "BDFxlAHQ7h-Y"
   },
   "outputs": [],
   "source": [
    "# Function to calculate all performance metrics\n",
    "\n",
    "def get_performance_metrics(y_df, pred_df, th, class_name):\n",
    "  y = y_df[class_name].values\n",
    "  pred = pred_df[class_name].values\n",
    "\n",
    "  def true_positives(y, pred, th):\n",
    "    TP = 0\n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "    # compute TP\n",
    "    TP = np.sum((y == 1) & (thresholded_preds == 1))\n",
    "    return TP\n",
    "\n",
    "  def true_negatives(y, pred, th):\n",
    "    TN = 0\n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "    # compute TN\n",
    "    TN = np.sum((y == 0) & (thresholded_preds == 0))\n",
    "    return TN\n",
    "\n",
    "  def false_positives(y, pred, th):\n",
    "    FP = 0\n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "    # compute FP\n",
    "    FP = np.sum((y == 0) & (thresholded_preds == 1)) \n",
    "    return FP\n",
    "\n",
    "  def false_negatives(y, pred, th):\n",
    "    FN = 0\n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th    \n",
    "    # compute FN\n",
    "    FN = np.sum((y == 1) & (thresholded_preds == 0))  \n",
    "    return FN\n",
    "\n",
    "  def get_accuracy(y, pred, th):\n",
    "    accuracy = 0.0\n",
    "    # get TP, FP, TN, FN using our previously defined functions\n",
    "    TP = true_positives(y, pred, th)\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "    # Compute accuracy using TP, FP, TN, FN\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return accuracy\n",
    "\n",
    "  def get_prevalence(y):\n",
    "    prevalence = 0.0\n",
    "    prevalence = np.sum(y == 1) / len(y) \n",
    "    return prevalence\n",
    "\n",
    "  def get_sensitivity(y, pred, th):\n",
    "    sensitivity = 0.0\n",
    "    # get TP and FN using our previously defined functions\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "    # use TP and FN to compute sensitivity\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    return sensitivity\n",
    "\n",
    "  def get_specificity(y, pred, th):\n",
    "    specificity = 0.0\n",
    "    # get TN and FP using our previously defined functions\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "    # use TN and FP to compute specificity \n",
    "    specificity = TN / (TN + FP)\n",
    "    return specificity\n",
    "\n",
    "  def get_ppv(y, pred, th):\n",
    "    PPV = 0.0\n",
    "    # get TP and FP using our previously defined functions\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "    # use TP and FP to compute PPV\n",
    "    PPV = TP / (TP + FP)\n",
    "    return PPV\n",
    "\n",
    "  def get_npv(y, pred, th):\n",
    "    NPV = 0.0\n",
    "    # get TN and FN using our previously defined functions\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "    # use TN and FN to compute NPV\n",
    "    NPV = TN / (TN + FN)\n",
    "    return NPV\n",
    "\n",
    "  def get_roc_auc(y, pred):\n",
    "    return roc_auc_score(y, pred)\n",
    "\n",
    "  def print_confidence_intervals(class_name, statistics):\n",
    "    print(\"CI for disease {} roc_auc_score:\".format(class_name))\n",
    "    print(np.percentile(statistics, 2.5), np.percentile(statistics, 97.5))\n",
    "\n",
    "  def bootstrap_auc(y, pred, class_name, bootstraps, fold_size):\n",
    "    statistics = np.zeros((1, bootstraps))\n",
    "    df = pd.DataFrame(columns=['y', 'pred'])\n",
    "    df.loc[:, 'y'] = y_df[class_name]\n",
    "    df.loc[:, 'pred'] = pred_df[class_name]\n",
    "    # get positive examples for stratified sampling\n",
    "    df_pos = df[df.y == 1]\n",
    "    df_neg = df[df.y == 0]\n",
    "    prevalence = len(df_pos) / len(df)\n",
    "    for i in range(bootstraps):\n",
    "      # stratified sampling of positive and negative examples\n",
    "      pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n",
    "      neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n",
    "\n",
    "      y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n",
    "      pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n",
    "      score = roc_auc_score(y_sample, pred_sample)\n",
    "      statistics[0][i] = score\n",
    "    print_confidence_intervals(class_name, statistics)\n",
    "    \n",
    "  def get_ap_score(y, pred):\n",
    "    return average_precision_score(y, pred)\n",
    "\n",
    "  def get_f1(y, pred):\n",
    "    precision = get_ppv(y, pred, th)\n",
    "    recall = get_sensitivity(y, pred, th)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return F1\n",
    "\n",
    "  accuracy = get_accuracy(y, pred, th)\n",
    "  prevalence = get_prevalence(y)\n",
    "  sensitivity = get_sensitivity(y, pred, th)\n",
    "  specificity = get_specificity(y, pred, th)\n",
    "  ppv = get_ppv(y, pred, th)\n",
    "  npv = get_npv(y, pred, th)\n",
    "  roc_auc = get_roc_auc(y, pred)\n",
    "  #bootstrap_auc(y, pred, class_name, bootstraps = 100, fold_size = len(y)) # PROBLEM\n",
    "  ap_score = get_ap_score(y, pred)   \n",
    "  f1_score = get_f1(y, pred)\n",
    "\n",
    "  return accuracy, prevalence, sensitivity, specificity, ppv, npv, roc_auc, ap_score, f1_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fpI5hMhRzaBV",
   "metadata": {
    "id": "fpI5hMhRzaBV"
   },
   "outputs": [],
   "source": [
    "# Iterate over class labels and calculate metrics\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "metrics_dict = dict()\n",
    "class_list = list()\n",
    "accuracy_list = list()\n",
    "prevalence_list = list() \n",
    "sensitivity_list = list() \n",
    "specificity_list = list() \n",
    "ppv_list = list() \n",
    "npv_list = list() \n",
    "roc_auc_list = list() \n",
    "ap_score_list = list() \n",
    "f1_score_list = list()\n",
    "\n",
    "for class_label in class_labels:\n",
    "  class_list.append(class_label)\n",
    "  metrics_dict['class'] = class_list\n",
    "  metrics_output = get_performance_metrics(y_df=correct_labels_df, pred_df=pred_df, th=threshold, class_name=class_label)\n",
    "  accuracy, prevalence, sensitivity, specificity, ppv, npv, roc_auc, ap_score, f1_score = metrics_output\n",
    "  accuracy_list.append(accuracy)\n",
    "  metrics_dict['accuracy'] = accuracy_list\n",
    "  prevalence_list.append(prevalence)\n",
    "  metrics_dict['prevalence'] = prevalence_list\n",
    "  sensitivity_list.append(sensitivity)\n",
    "  metrics_dict['sensitivity'] = sensitivity_list\n",
    "  specificity_list.append(specificity)\n",
    "  metrics_dict['specificity'] = specificity_list\n",
    "  ppv_list.append(ppv)\n",
    "  metrics_dict['ppv'] = ppv_list\n",
    "  npv_list.append(npv)\n",
    "  metrics_dict['npv'] = npv_list\n",
    "  roc_auc_list.append(roc_auc)\n",
    "  metrics_dict['roc_auc'] = roc_auc_list\n",
    "  ap_score_list.append(ap_score)\n",
    "  metrics_dict['ap_score'] = ap_score_list\n",
    "  f1_score_list.append(f1_score)\n",
    "  metrics_dict['f1_score'] = f1_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8LX7raaM0Cgw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LX7raaM0Cgw",
    "outputId": "c4dbd80e-2d75-40dc-e52f-804828007f1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 class  accuracy  prevalence  sensitivity  specificity  ppv  \\\n",
      "0            Emphysema      0.92        0.08          0.0          1.0  NaN   \n",
      "1           No Finding      0.86        0.14          0.0          1.0  NaN   \n",
      "2            Pneumonia      0.96        0.04          0.0          1.0  NaN   \n",
      "3         Infiltration      0.71        0.29          0.0          1.0  NaN   \n",
      "4               Nodule      0.89        0.11          0.0          1.0  NaN   \n",
      "5                Edema      0.94        0.06          0.0          1.0  NaN   \n",
      "6        Consolidation      0.87        0.13          0.0          1.0  NaN   \n",
      "7          Atelectasis      0.81        0.19          0.0          1.0  NaN   \n",
      "8         Pneumothorax      0.84        0.16          0.0          1.0  NaN   \n",
      "9                 Mass      0.88        0.12          0.0          1.0  NaN   \n",
      "10  Pleural_Thickening      0.92        0.08          0.0          1.0  NaN   \n",
      "11            Effusion      0.75        0.25          0.0          1.0  NaN   \n",
      "12        Cardiomegaly      0.93        0.07          0.0          1.0  NaN   \n",
      "13            Fibrosis      0.97        0.03          0.0          1.0  NaN   \n",
      "14              Hernia      0.99        0.01          0.0          1.0  NaN   \n",
      "\n",
      "     npv  roc_auc  ap_score  f1_score  \n",
      "0   0.92     0.50      0.08       NaN  \n",
      "1   0.86     0.50      0.14       NaN  \n",
      "2   0.96     0.48      0.04       NaN  \n",
      "3   0.71     0.50      0.29       NaN  \n",
      "4   0.89     0.50      0.12       NaN  \n",
      "5   0.94     0.49      0.06       NaN  \n",
      "6   0.87     0.50      0.12       NaN  \n",
      "7   0.81     0.50      0.19       NaN  \n",
      "8   0.84     0.51      0.16       NaN  \n",
      "9   0.88     0.50      0.12       NaN  \n",
      "10  0.92     0.50      0.08       NaN  \n",
      "11  0.75     0.50      0.25       NaN  \n",
      "12  0.93     0.50      0.07       NaN  \n",
      "13  0.97     0.52      0.03       NaN  \n",
      "14  0.99     0.48      0.01       NaN  \n"
     ]
    }
   ],
   "source": [
    "# Summary metrics each class label on test set\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "print(metrics_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rSeZwjBr07K4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "rSeZwjBr07K4",
    "outputId": "622692c1-9de4-43e5-f9f4-3b014dd405a1"
   },
   "outputs": [],
   "source": [
    "# AUROC for all classes\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for class_label in class_labels:\n",
    "  y = correct_labels_df[class_label].values\n",
    "  pred = pred_df[class_label].values\n",
    "  fpr, tpr, thresholds = roc_curve(y, pred)\n",
    "  auc = roc_auc_score(y, pred)\n",
    "  plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (class_label, auc))\n",
    "# Custom settings for the plot \n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity (False Positive Rate)')\n",
    "plt.ylabel('Sensitivity (True Positive Rate)')\n",
    "plt.title('Area Under Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aKBSV9Pc1LaL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "aKBSV9Pc1LaL",
    "outputId": "21c0200d-c139-4079-bc83-3d47a0bf9f80"
   },
   "outputs": [],
   "source": [
    "# Precision Recall Curve for all classes\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for class_label in class_labels:\n",
    "  y = correct_labels_df[class_label].values\n",
    "  pred = pred_df[class_label].values\n",
    "  p, r, t = precision_recall_curve(y, pred)\n",
    "  ap_score = average_precision_score(y, pred)\n",
    "  plt.plot(r, p, label='%s PRC (ap = %0.2f)' % (class_label, ap_score))\n",
    "# Custom settings for the plot \n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b17a01-1424-4ae6-9cba-5cba006346d6",
   "metadata": {},
   "source": [
    "<a name=\"5.3.-Variable-Importance\"></a>\n",
    "## 5.3 Variable Importance\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F5K6XiVPMwGK",
   "metadata": {
    "id": "F5K6XiVPMwGK",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"6.-Conclusions\"></a>\n",
    "# 6. Conclusions\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A3j6FUJ5UurJ",
   "metadata": {
    "id": "A3j6FUJ5UurJ"
   },
   "source": [
    "We found this project to be both interesting and quite challenging. Blah blah blah.\n",
    "\n",
    "Perhaps the three most perplexing issues we encountered were:\n",
    "\n",
    "1. placeholder1 \n",
    "\n",
    "2. placeholder2\n",
    "\n",
    "3. placeholder3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f22514-5851-41d7-b76f-fe8f31aa9b86",
   "metadata": {},
   "source": [
    "<a name=\"6.1.-Patient-Early-Readmittance-Rate\"></a>\n",
    "## 6.1 Patient Early Readmittance Rate\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ff102-2c83-4689-a739-17b2c91b7404",
   "metadata": {},
   "source": [
    "<a name=\"6.2.-Patient-Risk-Profiles\"></a>\n",
    "## 6.2 Patient Risk Profiles\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tBzglpfuMzCx",
   "metadata": {
    "id": "tBzglpfuMzCx",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"7.-Future-Work\"></a>\n",
    "# 7. Future Work\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3y_O1P60Y0N",
   "metadata": {
    "id": "b3y_O1P60Y0N"
   },
   "source": [
    "In our analysis of the Chest X-ray dataset we have tried a variety of modeling and feature engineering approaches, but there are still several additional steps that could be taken:\n",
    "\n",
    "1. placeholder1\n",
    "\n",
    "2. placeholder2\n",
    "\n",
    "3. placeholder2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JIM0q5KDxhvF",
   "metadata": {
    "id": "JIM0q5KDxhvF",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"8.-References\"></a>\n",
    "# 8. References\n",
    "\n",
    "[Return to top](#Notebook-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iFVg1gboxmA6",
   "metadata": {
    "id": "iFVg1gboxmA6"
   },
   "source": [
    "**The following are links to papers, blogs, and tutorials we found useful during the development of this project:**\n",
    "\n",
    "Fine-tuning for transfer learning models:\n",
    "https://keras.io/guides/transfer_learning/\n",
    "\n",
    "Medical neural networks:\n",
    "https://glassboxmedicine.com/\n",
    "\n",
    "Image classification using CNNs:\n",
    "https://towardsdatascience.com/medical-x-ray-%EF%B8%8F-image-classification-using-convolutional-neural-network-9a6d33b1c2a\n",
    "\n",
    "Comparison of ResNet50 and VGG19 and training from stratch for X-ray images dataset:\n",
    "https://www.sciencedirect.com/science/article/pii/S2666285X21000558\n",
    "\n",
    "Tensorflow Applications for base model:\n",
    "https://keras.io/api/applications/\n",
    "\n",
    "Tensorboard confusion matrix:\n",
    "https://towardsdatascience.com/exploring-confusion-matrix-evolution-on-tensorboard-e66b39f4ac12\n",
    "\n",
    "Pre-processing and modeling pipelines (ResNet50):\n",
    "https://towardsdatascience.com/time-to-choose-tensorflow-data-over-imagedatagenerator-215e594f2435\n",
    "\n",
    "Image data input pipelines:\n",
    "https://towardsdatascience.com/what-is-the-best-input-pipeline-to-train-image-classification-models-with-tf-keras-eb3fe26d3cc5\n",
    "\n",
    "Split TF datasets:\n",
    "https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\n",
    "\n",
    "Transfer learning with EfficientNet:\n",
    "https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n",
    "\n",
    "Training greyscale images using transfer learning:\n",
    "https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images\n",
    "\n",
    "Multi-label vs multi-class classification:\n",
    "https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/\n",
    "\n",
    "Multi-label classification example use-case:\n",
    "https://towardsdatascience.com/fast-ai-season-1-episode-3-a-case-of-multi-label-classification-a4a90672a889\n",
    "\n",
    "Element-wise sigmoid:\n",
    "https://www.programcreek.com/python/example/93769/keras.backend.sigmoid\n",
    "\n",
    "Element-wise sigmoid:\n",
    "https://stackoverflow.com/questions/52090857/how-to-apply-sigmoid-function-for-each-outputs-in-keras\n",
    "\n",
    "DenseNet121:\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8189817/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "final_report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
